import streamlit as st
import requests
import pandas as pd
import time
import random
from bs4 import BeautifulSoup
import io

st.set_page_config(page_title="HH Full Scraper (fixed)", layout="wide")
st.title("üî• HH FULL Vacancy Scraper ‚Äî –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è")

# ---------------------------
# INPUTS
# ---------------------------
title_keywords_input = st.text_area(
    "üîç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
    value="–ø—Ä–æ–¥—É–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä, product manager, –ø—Ä–æ–¥–∞–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä, –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"
)

title_exclude_input = st.text_area(
    "üö´ –ò—Å–∫–ª—é—á–∏—Ç—å —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
    value="—Å—Ç–∞–∂–µ—Ä, intern"
)

desc_keywords_input = st.text_area(
    "üìå –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
    value="Firebase,Amplitude"
)

desc_exclude_input = st.text_area(
    "üö´ –ò—Å–∫–ª—é—á–∏—Ç—å —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
    value="1C, –≤–æ–¥–∏—Ç–µ–ª—å"
)

desc_mode = st.radio(
    "–ö–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏?",
    ["–•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ", "–í—Å–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞—Å—Ç—å"]
)

# normalize lists
title_keywords = [t.strip().lower() for t in title_keywords_input.split(",") if t.strip()]
title_exclude = [t.strip().lower() for t in title_exclude_input.split(",") if t.strip()]
desc_keywords = [t.strip().lower() for t in desc_keywords_input.split(",") if t.strip()]
desc_exclude = [t.strip().lower() for t in desc_exclude_input.split(",") if t.strip()]

# ---------------------------
# helper: fetch whole description with multiple fallbacks
# ---------------------------
def fetch_full_description(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code != 200:
            return ""
        soup = BeautifulSoup(r.text, "lxml")

        texts = []

        # 1) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä hh
        c = soup.find("div", {"data-qa": "vacancy-description"})
        if c:
            texts.append(c.get_text(" ", strip=True))

        # 2) –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è
        for cls in ["g-user-content", "vacancy-section", "vacancy-description__text"]:
            alt = soup.find("div", class_=cls)
            if alt:
                texts.append(alt.get_text(" ", strip=True))

        # 3) –≤—Å–µ <section> –∏–ª–∏ <article> –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (fallback)
        for tag in soup.find_all(["article", "section"]):
            texts.append(tag.get_text(" ", strip=True))

        # 4) meta description (–µ—Å–ª–∏ –µ—Å—Ç—å)
        meta = soup.find("meta", {"name": "description"})
        if meta and meta.get("content"):
            texts.append(meta["content"])

        # 5) –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å ‚Äî –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if not texts:
            texts.append(soup.get_text(" ", strip=True))

        merged = " ".join([t for t in texts if t]).replace("\n", " ")
        merged = " ".join(merged.split())  # normalize spaces
        return merged.lower()
    except Exception:
        return ""

# ---------------------------
# helper: get pages count for a keyword (single quick request)
# ---------------------------
def get_pages_for_keyword(api_url, keyword, area_id, per_page):
    try:
        params = {"text": keyword, "area": area_id, "per_page": per_page, "page": 0}
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(api_url, params=params, headers=headers, timeout=10)
        if r.status_code != 200:
            return 0, 0  # pages, found
        data = r.json()
        pages = data.get("pages", 0) or 0
        found = data.get("found", 0) or 0
        return pages, found
    except Exception:
        return 0, 0

# ---------------------------
# MAIN SEARCH
# ---------------------------
if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–∏—Å–∫"):

    # basic config
    hh_api = "https://api.hh.kz/vacancies"
    per_page = 100
    area_id = 160  # –ê–ª–º–∞—Ç—ã

    st.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞... —Å–æ–±–∏—Ä–∞—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    status = st.empty()
    found_counter = st.empty()

    # 1) –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ ‚Äî —É–∑–Ω–∞–µ–º –æ–±—â–µ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª—é—á—É
    total_pages = 0
    pages_for_key = {}
    found_for_key = {}
    for kw in title_keywords:
        pgs, found = get_pages_for_keyword(hh_api, kw, area_id, per_page)
        # guard: pages may be large; ensure int
        pgs = int(pgs) if isinstance(pgs, (int, float)) else 0
        found = int(found) if isinstance(found, (int, float)) else 0
        pages_for_key[kw] = max(pgs, 1)  # at least 1 to process page0
        found_for_key[kw] = found
        total_pages += pages_for_key[kw]
        time.sleep(random.uniform(0.15, 0.3))  # light throttle

    if total_pages == 0:
        total_pages = 1

    status.info(f"–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    progress = st.progress(0.0)

    # 2) —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ö–æ–¥, —Å —Ç–æ—á–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
    vacancies = []
    seen_ids = set()
    processed_pages = 0

    for kw in title_keywords:
        max_pages_for_kw = pages_for_key.get(kw, 1)
        current_page = 0

        while current_page < max_pages_for_kw:
            status.write(f"üîé –ö–ª—é—á: **{kw}** ‚Äî —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page+1}/{max_pages_for_kw}")
            params = {"text": kw, "area": area_id, "per_page": per_page, "page": current_page}
            headers = {"User-Agent": "Mozilla/5.0"}

            try:
                r = requests.get(hh_api, params=params, headers=headers, timeout=12)
                # handle transient server errors
                if r.status_code in (429, 500, 502, 503, 504):
                    status.warning(f"–°–µ—Ä–≤–µ—Ä –æ—Ç–≤–µ—Ç–∏–ª {r.status_code}, –ø–æ–¥–æ–∂–¥—ë–º –∏ –ø–æ–ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞")
                    time.sleep(random.uniform(1.0, 2.5))
                    continue
                if r.status_code != 200:
                    status.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {r.status_code} (–ø—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É)")
                    break

                data = r.json()
                items = data.get("items", [])
                if not items:
                    # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ API —Å–∫–∞–∑–∞–ª, —á—Ç–æ pages>0, –Ω–æ items –ø—É—Å—Ç—ã–µ ‚Äî –≤—ã–π–¥–µ–º —Å –¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    break

                for vac in items:
                    # take id and ensure uniqueness
                    vac_id = vac.get("id")
                    if vac_id in seen_ids:
                        continue
                    seen_ids.add(vac_id)

                    title = (vac.get("name") or "").lower()

                    # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                    if title_keywords and not any(tok in title for tok in title_keywords):
                        continue
                    if title_exclude and any(ex in title for ex in title_exclude):
                        continue

                    # –ø–æ–¥–≥—Ä—É–∂–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º desc-—Ñ–∏–ª—å—Ç—Ä—ã
                    url = vac.get("alternate_url") or ""
                    full_desc = fetch_full_description(url)

                    if desc_exclude and any(ex in full_desc for ex in desc_exclude):
                        continue

                    if desc_keywords:
                        if desc_mode == "–•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ":
                            if not any(k in full_desc for k in desc_keywords):
                                continue
                        else:
                            if not all(k in full_desc for k in desc_keywords):
                                continue

                    # –∞–¥—Ä–µ—Å
                    addr = vac.get("address") or {}
                    street = addr.get("street") or ""
                    building = addr.get("building") or ""
                    address = f"{street} {building}".strip() or "-"

                    # 2GIS link
                    if address != "-":
                        query = f"–ê–ª–º–∞—Ç—ã, {address}".replace(" ", "+")
                        address_link = f"https://2gis.kz/almaty/search/{query}"
                    else:
                        address_link = "-"

                    vacancies.append({
                        "ID": vac_id,
                        "–ù–∞–∑–≤–∞–Ω–∏–µ": vac.get("name") or "-",
                        "–ö–æ–º–ø–∞–Ω–∏—è": (vac.get("employer") or {}).get("name") or "-",
                        "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏": (vac.get("published_at") or "-")[:10],
                        "–ê–¥—Ä–µ—Å": address,
                        "–°—Å—ã–ª–∫–∞ HH": url or "-",
                        "–°—Å—ã–ª–∫–∞ 2GIS": address_link
                    })

                # –∑–∞–≤–µ—Ä—à–∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É
                current_page += 1
                processed_pages += 1
                # –æ–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (—Ç–æ—á–Ω–æ –ø–æ —á–∏—Å–ª—É —Å—Ç—Ä–∞–Ω–∏—Ü)
                progress.progress(min(processed_pages / total_pages, 1.0))
                found_counter.markdown(f"**–ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π:** {len(vacancies)}")
                # –Ω–µ–±–æ–ª—å—à–∞—è —Å–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                time.sleep(random.uniform(0.2, 0.6))

            except Exception as e:
                status.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}. –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
                current_page += 1
                processed_pages += 1
                progress.progress(min(processed_pages / total_pages, 1.0))
                time.sleep(0.5)
                continue

    status.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    st.write(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: **{len(vacancies)}**")

    # ---------------------------
    # OUTPUT
    # ---------------------------
    if vacancies:
        df = pd.DataFrame(vacancies)

        # —Å–æ–∑–¥–∞—ë–º –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ HTML —Å—Å—ã–ª–∫–∏ (–¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü–µ)
        df_display = df.copy()
        df_display["–°—Å—ã–ª–∫–∞ HH"] = df_display["–°—Å—ã–ª–∫–∞ HH"].apply(
            lambda x: f'<a href="{x}" target="_blank">üîó –û—Ç–∫—Ä—ã—Ç—å HH</a>' if x and x != "-" else "-"
        )
        df_display["–°—Å—ã–ª–∫–∞ 2GIS"] = df_display["–°—Å—ã–ª–∫–∞ 2GIS"].apply(
            lambda x: f'<a href="{x}" target="_blank">üìç 2GIS</a>' if x and x != "-" else "-"
        )

        # –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è HTML-—Ç–∞–±–ª–∏—Ü–∞ —Å–æ —Å—Ç–∏–ª—è–º–∏ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ –ù–ï –±–µ–ª—ã–π)
        table_html = df_display.to_html(escape=False, index=False)
        styled = f"""
        <style>
         thead th {{ background:#1f2937; color:#fff; padding:8px; position: sticky; top:0; z-index:1; }}
         table {{ border-collapse: collapse; width:100%; font-family: Arial, sans-serif; }}
         td, th {{ border: 1px solid #ddd; padding: 8px; text-align:left; vertical-align: top; }}
         tbody tr:nth-child(odd){{ background:#fbfbfb; }}
         tbody tr:hover{{ background:#eef6ff; }}
         a {{ text-decoration:none; color: #0645AD; font-weight:600; }}
        </style>
        {table_html}
        """

        st.markdown(styled, unsafe_allow_html=True)

        # Excel (—á–∏—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –±–µ–∑ HTML)
        excel_buffer = io.BytesIO()
        pd.DataFrame(vacancies).to_excel(excel_buffer, index=False)
        excel_buffer.seek(0)

        st.download_button(
            label="‚¨á –°–∫–∞—á–∞—Ç—å Excel",
            data=excel_buffer,
            file_name="vacancies.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.info("–ü–æ –∑–∞–¥–∞–Ω–Ω—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º –≤–∞–∫–∞–Ω—Å–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
