import streamlit as st
import requests
import pandas as pd
import time
from bs4 import BeautifulSoup
import io

st.set_page_config(page_title="HH Full Scraper", layout="wide")
st.title("üî• HH FULL Vacancy Scraper ‚Äî —É–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å + –ø—Ä–æ–≥—Ä–µ—Å—Å")

# ---------------------------
# INPUTS
# ---------------------------
title_keywords_input = st.text_area(
    "üîç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏:",
    value="–ø—Ä–æ–¥—É–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä, product manager, –ø—Ä–æ–¥–∞–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä, –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"
)

title_exclude_input = st.text_area(
    "üö´ –ò—Å–∫–ª—é—á–∏—Ç—å —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏:",
    value="—Å—Ç–∞–∂–µ—Ä, intern"
)

desc_keywords_input = st.text_area(
    "üìå –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏:",
    value="Firebase,Amplitude"
)

desc_exclude_input = st.text_area(
    "üö´ –ò—Å–∫–ª—é—á–∏—Ç—å —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏:",
    value="1C, –≤–æ–¥–∏—Ç–µ–ª—å"
)

desc_mode = st.radio(
    "–ö–∞–∫ –∏—Å–∫–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏?",
    ["–•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ", "–í—Å–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞—Å—Ç—å"]
)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤
title_keywords = [t.strip().lower() for t in title_keywords_input.split(",") if t.strip()]
title_exclude = [t.strip().lower() for t in title_exclude_input.split(",") if t.strip()]
desc_keywords = [t.strip().lower() for t in desc_keywords_input.split(",") if t.strip()]
desc_exclude = [t.strip().lower() for t in desc_exclude_input.split(",") if t.strip()]

# ---------------------------
# HTML –æ–ø–∏—Å–∞–Ω–∏—è
# ---------------------------
def fetch_full_description(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        html = requests.get(url, headers=headers, timeout=10)

        if html.status_code != 200:
            return ""

        soup = BeautifulSoup(html.text, "lxml")
        block = soup.find("div", {"data-qa": "vacancy-description"})
        if not block:
            return ""

        text = block.get_text(separator=" ", strip=True)
        text = " ".join(text.split())
        return text.lower()

    except:
        return ""


# ---------------------------
# RUN SEARCH
# ---------------------------
if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–∏—Å–∫"):

    st.info("üîÑ –ü–æ–∏—Å–∫ –Ω–∞—á–∞–ª—Å—è...")

    progress = st.progress(0)
    status_text = st.empty()
    found_counter = st.empty()

    hh_api = "https://api.hh.kz/vacancies"
    per_page = 100
    area_id = 160  # –ê–ª–º–∞—Ç—ã

    vacancies = []
    seen_ids = set()

    total_steps = len(title_keywords) * 50  # —É—Å–ª–æ–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    step = 0

    for keyword in title_keywords:

        page = 0
        status_text.write(f"üîé –ò—â—É –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: **{keyword}** (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page})")

        while True:

            params = {"text": keyword, "area": area_id, "per_page": per_page, "page": page}
            headers = {"User-Agent": "Mozilla/5.0"}

            try:
                r = requests.get(hh_api, params=params, headers=headers, timeout=10)
                if r.status_code != 200:
                    time.sleep(1)
                    continue

                data = r.json()
                items = data.get("items", [])

                if not items:
                    break

                for vac in items:

                    vac_id = vac.get("id")
                    if vac_id in seen_ids:
                        continue
                    seen_ids.add(vac_id)

                    title = vac.get("name", "").lower()

                    # —Ñ–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
                    if not any(k in title for k in title_keywords):
                        continue
                    if any(ex in title for ex in title_exclude):
                        continue

                    # –ø–æ–¥–≥—Ä—É–∑–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è
                    url = vac.get("alternate_url", "")
                    desc = fetch_full_description(url)

                    # —Ñ–∏–ª—å—Ç—Ä –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é
                    if any(ex in desc for ex in desc_exclude):
                        continue

                    if desc_keywords:
                        if desc_mode == "–•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ":
                            if not any(k in desc for k in desc_keywords):
                                continue
                        else:
                            if not all(k in desc for k in desc_keywords):
                                continue

                    # –∞–¥—Ä–µ—Å
                    addr = vac.get("address")
                    address = "-"
                    if addr:
                        street = addr.get("street", "")
                        building = addr.get("building", "")
                        address = f"{street} {building}".strip() or "-"

                    # —Å—Å—ã–ª–∫–∞ 2GIS
                    if address != "-":
                        query = f"–ê–ª–º–∞—Ç—ã {address}".replace(" ", "+")
                        g2 = f"https://2gis.kz/almaty/search/{query}"
                    else:
                        g2 = "-"

                    vacancies.append({
                        "ID": vac_id,
                        "–ù–∞–∑–≤–∞–Ω–∏–µ": vac.get("name", "-"),
                        "–ö–æ–º–ø–∞–Ω–∏—è": vac.get("employer", {}).get("name", "-"),
                        "–î–∞—Ç–∞": vac.get("published_at", "-")[:10],
                        "–ê–¥—Ä–µ—Å": address,
                        "HH": url,
                        "2GIS": g2
                    })

                    found_counter.write(f"üìå –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: **{len(vacancies)}**")

                # progress bar
                step += 1
                progress.progress(min(step / total_steps, 1.0))

                page += 1
                status_text.write(f"üîé –ò—â—É –ø–æ –∫–ª—é—á—É **{keyword}**, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}")

                time.sleep(0.3)

            except:
                break

    st.success(f"–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω! –ù–∞–π–¥–µ–Ω–æ: {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π.")

    # ---------------------------
    # OUTPUT TABLE (–∫—Ä–∞—Å–∏–≤–æ)
    # ---------------------------
    if vacancies:

        df = pd.DataFrame(vacancies)

        # –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        df["HH"] = df["HH"].apply(lambda x: f'<a href="{x}" target="_blank">üîó HH</a>' if x != "-" else "-")
        df["2GIS"] = df["2GIS"].apply(lambda x: f'<a href="{x}" target="_blank">üìç 2GIS</a>' if x != "-" else "-")

        # —Å—Ç–∏–ª–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã
        table_html = df.to_html(escape=False, index=False)

        styled_html = f"""
        <style>
        table {{
            width: 100%;
            border-collapse: collapse;
            font-family: Arial, sans-serif;
            font-size: 15px;
        }}
        thead th {{
            background: #2c2f33;
            color: white;
            padding: 10px;
            position: sticky;
            top: 0;
            z-index: 1;
        }}
        tbody tr:nth-child(odd) {{ background: #f5f5f5; }}
        tbody tr:nth-child(even) {{ background: #ffffff; }}
        td {{
            padding: 8px;
            border: 1px solid #ddd;
        }}
        tbody tr:hover {{
            background: #e1ecff;
        }}
        a {{
            text-decoration: none;
            font-weight: bold;
        }}
        </style>
        {table_html}
        """

        st.markdown(styled_html, unsafe_allow_html=True)

        # Excel (—á–∏—Å—Ç—ã–π)
        excel_buffer = io.BytesIO()
        pd.DataFrame(vacancies).to_excel(excel_buffer, index=False)
        excel_buffer.seek(0)

        st.download_button(
            label="‚¨á –°–∫–∞—á–∞—Ç—å Excel",
            data=excel_buffer,
            file_name="vacancies.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
