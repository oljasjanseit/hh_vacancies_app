import streamlit as st
import requests
import pandas as pd
import time
from bs4 import BeautifulSoup
import io

st.set_page_config(page_title="HH FULL Vacancy Scraper", layout="wide")
st.title("HH FULL Vacancy Scraper (Title + Description Parsing)")

# ---------------------------
# –í–í–û–î –ü–ê–†–ê–ú–ï–¢–†–û–í
# ---------------------------
title_keywords_input = st.text_area(
    "–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
    value="–ø—Ä–æ–¥—É–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä, product manager"
)

title_exclude_input = st.text_area(
    "–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
    value="—Å—Ç–∞–∂–µ—Ä,intern"
)

desc_keywords_input = st.text_area(
    "–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
    value="Firebase,Amplitude"
)

desc_exclude_input = st.text_area(
    "–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):",
    value="1C,–≤–æ–¥–∏—Ç–µ–ª—å"
)

desc_mode = st.radio(
    "–ö–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏?",
    ["–•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ", "–í—Å–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞—Å—Ç—å"]
)

# ---------------------------
# –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï –°–ü–ò–°–ö–û–í
# ---------------------------
title_keywords = [t.strip().lower() for t in title_keywords_input.split(",") if t.strip()]
title_exclude = [t.strip().lower() for t in title_exclude_input.split(",") if t.strip()]
desc_keywords = [t.strip().lower() for t in desc_keywords_input.split(",") if t.strip()]
desc_exclude = [t.strip().lower() for t in desc_exclude_input.split(",") if t.strip()]

# ---------------------------
# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
# ---------------------------
def fetch_full_description(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        html = requests.get(url, headers=headers, timeout=8)
        if html.status_code != 200:
            return ""
        soup = BeautifulSoup(html.text, "lxml")
        container = soup.find("div", {"data-qa": "vacancy-description"})
        if not container:
            return ""
        text = container.get_text(separator=" ", strip=True)
        text = " ".join(text.replace("\n", " ").split())
        return text.lower()
    except:
        return ""

# ---------------------------
# –ó–ê–ü–£–°–ö –ü–û–ò–°–ö–ê
# ---------------------------
if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–∏—Å–∫"):

    st.info("–ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫‚Ä¶")
    vacancies = []
    seen_ids = set()

    hh_api = "https://api.hh.ru/vacancies"
    per_page = 100
    area_id = 160  # –ê–ª–º–∞—Ç—ã

    progress_bar = st.progress(0)
    status_text = st.empty()

    total_keywords = len(title_keywords)
    keyword_index = 0

    for keyword in title_keywords:
        keyword_index += 1
        page = 0
        while True:
            params = {"text": keyword, "area": area_id, "per_page": per_page, "page": page}
            headers = {"User-Agent": "Mozilla/5.0"}
            try:
                r = requests.get(hh_api, params=params, headers=headers, timeout=10)
                if r.status_code != 200:
                    time.sleep(1)
                    continue
                data = r.json()
                items = data.get("items", [])
                if not items:
                    break
                for vac in items:
                    vac_id = vac.get("id")
                    title = vac.get("name", "").lower()
                    # --- —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é ---
                    if not any(k in title for k in title_keywords):
                        continue
                    if any(ex in title for ex in title_exclude):
                        continue
                    # --- –∑–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–µ–π ---
                    if vac_id in seen_ids:
                        continue
                    seen_ids.add(vac_id)
                    # --- –ì—Ä—É–∑–∏–º –ø–æ–ª–Ω—ã–π HTML ---
                    url = vac.get("alternate_url", "")
                    full_desc = fetch_full_description(url)
                    # --- —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é ---
                    if desc_exclude and any(ex in full_desc for ex in desc_exclude):
                        continue
                    if desc_keywords:
                        if desc_mode == "–•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ":
                            if not any(k in full_desc for k in desc_keywords):
                                continue
                        else:
                            if not all(k in full_desc for k in desc_keywords):
                                continue
                    addr = vac.get("address")
                    address = "-"
                    if addr:
                        street = addr.get("street", "")
                        building = addr.get("building", "")
                        address = f"{street} {building}".strip() or "-"
                    # --- –°—Å—ã–ª–∫–∞ –Ω–∞ 2GIS ---
                    if address != "-":
                        query = f"–ê–ª–º–∞—Ç—ã, {address}".replace(" ", "+")
                        address_link = f"https://2gis.kz/almaty/search/{query}"
                    else:
                        address_link = "-"
                    vacancies.append({
                        "ID": vac_id,
                        "–ù–∞–∑–≤–∞–Ω–∏–µ": vac.get("name", "-"),
                        "–ö–æ–º–ø–∞–Ω–∏—è": vac.get("employer", {}).get("name", "-"),
                        "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏": vac.get("published_at", "-")[:10],
                        "–ê–¥—Ä–µ—Å": address,
                        "–°—Å—ã–ª–∫–∞ HH": url,
                        "–°—Å—ã–ª–∫–∞ 2GIS": address_link
                    })
                page += 1
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress_value = min((keyword_index - 1 + page/50)/total_keywords, 1.0)
                progress_bar.progress(progress_value)
                status_text.text(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ {keyword_index}/{total_keywords}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}, –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞–π–¥–µ–Ω–æ: {len(vacancies)}")
                time.sleep(0.2)
            except:
                break

    st.success(f"–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω! –ù–∞–π–¥–µ–Ω–æ {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π.")

    if vacancies:
        df = pd.DataFrame(vacancies)

        # –ö–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        df_display = df.copy()
        df_display["–°—Å—ã–ª–∫–∞ HH"] = df_display["–°—Å—ã–ª–∫–∞ HH"].apply(lambda x: f'<a href="{x}" target="_blank">üîó –û—Ç–∫—Ä—ã—Ç—å HH</a>' if x != "-" else "-")
        df_display["–°—Å—ã–ª–∫–∞ 2GIS"] = df_display["–°—Å—ã–ª–∫–∞ 2GIS"].apply(lambda x: f'<a href="{x}" target="_blank">üìç 2GIS</a>' if x != "-" else "-")

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ markdown —Ç–∞–±–ª–∏—Ü—É —Å HTML
        table_html = "<table style='width:100%; border-collapse: collapse;'>"
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        table_html += "<tr>"
        for col in df_display.columns:
            table_html += f"<th style='border: 1px solid #ccc; padding: 6px;'>{col}</th>"
        table_html += "</tr>"
        # –î–∞–Ω–Ω—ã–µ
        for _, row in df_display.iterrows():
            table_html += "<tr>"
            for col in df_display.columns:
                table_html += f"<td style='border: 1px solid #ccc; padding: 6px;'>{row[col]}</td>"
            table_html += "</tr>"
        table_html += "</table>"

        st.markdown(table_html, unsafe_allow_html=True)

        # –í—ã–≥—Ä—É–∑–∫–∞ Excel
        excel_buffer = io.BytesIO()
        df.to_excel(excel_buffer, index=False)
        excel_buffer.seek(0)
        st.download_button(
            label="–°–∫–∞—á–∞—Ç—å Excel",
            data=excel_buffer,
            file_name="vacancies.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
